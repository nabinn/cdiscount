{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras.preprocessing.image as kimage\n",
    "from keras.models import load_model\n",
    "#import pickle\n",
    "\n",
    "#from common import *\n",
    "#from torch.autograd import Variable\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from excited_inception_v3 import SEInception3\n",
    "#from inception_v3 import Inception3\n",
    "#from xception import Xception\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILELIST_CSV =\"FileList.csv\"\n",
    "\n",
    "\n",
    "CATEGORIES_CSV=\"categories.csv\" \n",
    "category_ids = np.array(pd.read_csv(CATEGORIES_CSV, header=None).values).flatten()\n",
    "category_ids = [str(item) for item in category_ids]\n",
    "\n",
    "#CATEGORIES_PKL='class_order.pkl'\n",
    "#category_ids = pickle.load(open(CATEGORIES_PKL, \"rb\"))\n",
    "\n",
    "#INRESV2_CATEGORIES=\"categories_incepResv2.csv\"\n",
    "#category_ids = np.array(pd.read_csv(INRESV2_CATEGORIES, header=None).values).flatten()\n",
    "#category_ids = [str(item) for item in category_ids]\n",
    "\n",
    "INCLUDE_FLIPPED_IMAGES=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result_ids=np.array([], dtype=int)\n",
    "#result_classes=np.array([], dtype=int)\n",
    "\n",
    "buffer_pred=np.array([], dtype=np.float32) #buffer to keep temporary predictios\n",
    "buffer_id=np.array([], dtype=np.float32) # buffers to keep temporary ids\n",
    "\n",
    "#result_csv=\"Inceptionv3pytorch_result.csv\"\n",
    "result_predictions=\"ensemble.h5\"\n",
    "\n",
    "#result_csv = os.path.join(\"/home/cvpr/Desktop/Nabin/cdiscount/\", result_csv)\n",
    "result_predictions=os.path.join(\"/home/cvpr/Desktop/Nabin/cdiscount/ensemble\", result_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "hdf5_file = tables.open_file(result_predictions, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_mean = hdf5_file.create_earray(hdf5_file.root, 'keras_xcp_mean', \n",
    "                                           tables.Float32Atom(), shape=(0, 5270),\n",
    "                                          expectedrows=1768182)\n",
    "predictions_median = hdf5_file.create_earray(hdf5_file.root, 'keras_xcp_median', \n",
    "                                           tables.Float32Atom(), shape=(0, 5270),\n",
    "                                          expectedrows=1768182)\n",
    "predictions_max = hdf5_file.create_earray(hdf5_file.root, 'keras_xcp_max', \n",
    "                                           tables.Float32Atom(), shape=(0, 5270),\n",
    "                                          expectedrows=1768182)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDiscountKeras(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, img_size=(180, 180)):\n",
    "        '''\n",
    "        Parameters:\n",
    "            csv_file: a csv file with all details about images\n",
    "            img_size: a tuple (image rows, image columns)\n",
    "        '''\n",
    "        super(CDiscountKeras, self).__init__()\n",
    "        self.csv_file  = pd.read_csv(csv_file)\n",
    "        self.img_files = self.csv_file[\"file_name\"].values\n",
    "        self.ids = self.csv_file[\"id\"].values\n",
    "        self.img_nums=self.csv_file[\"img_num\"].values\n",
    "        self.img_size=img_size\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.img_files[index]\n",
    "        img_id = self.ids[index]\n",
    "        img_num = self.img_nums[index]\n",
    "        \n",
    "        image = kimage.load_img(filename,target_size=self.img_size)\n",
    "        image = kimage.img_to_array(image)\n",
    "        image = self.transform1(image)\n",
    "        \n",
    "        return image, img_id, img_num\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    \n",
    "    def transform1(self,x, reverse_mode=False):\n",
    "        '''\n",
    "        converts values from [0,255] \n",
    "        to [0, 1] and vice versa\n",
    "                Parameter:\n",
    "            reverse_mode: boolean, default is False\n",
    "                          False:[0, 255]-->[0, 1] \n",
    "                          True: [0, 1]-->[0, 255]\n",
    "        '''\n",
    "        if reverse_mode:\n",
    "            return x * 255.\n",
    "        else:\n",
    "            return x/255.\n",
    "    \n",
    "    def transform2(self,x, reverse_mode=False):\n",
    "        '''\n",
    "        converts values from [0,255] \n",
    "        to [-1, 1] and vice versa\n",
    "        \n",
    "        Parameter:\n",
    "            reverse_mode: boolean, default is False\n",
    "                          False:[0, 255]-->[-1, 1] \n",
    "                          True: [-1, 1]-->[0, 255]\n",
    "        '''\n",
    "        if reverse_mode:\n",
    "            return ((x / 2.0)+0.5)*255.0\n",
    "        else:\n",
    "            return ((x / 255.0)-0.5)*2.0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdiscount_test_data=CDiscountKeras(csv_file=FILELIST_CSV)\n",
    "dataloader = DataLoader(cdiscount_test_data, batch_size=256, num_workers=16)\n",
    "data=iter(dataloader)\n",
    "total_batches = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('xception_v2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_buffer(data=None, index=None):\n",
    "    pred_df = pd.DataFrame(data=data,  index=index, columns= category_ids, dtype=np.float32)\n",
    "    #merge rows with same index (product id)\n",
    "    pred_df_mean = pred_df.groupby(pred_df.index).mean()\n",
    "    pred_df_median = pred_df.groupby(pred_df.index).median()\n",
    "    pred_df_max = pred_df.groupby(pred_df.index).max()\n",
    "    \n",
    "    predictions_mean.append(pred_df_mean.values)\n",
    "    predictions_median.append(pred_df_median.values)\n",
    "    predictions_max.append(pred_df_max.values)\n",
    "    #hdf5_ids.append(pred_df.index.values.reshape(-1, 1))\n",
    "    \n",
    "    #find the column with maximum predicted value\n",
    "    #pred_df[\"category_id\"] = pred_df.idxmax(axis=1)\n",
    "\n",
    "    #result_ids = pred_df.index.tolist()\n",
    "    #result_classes = pred_df[\"category_id\"].values\n",
    "    #return result_ids, result_classes\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815447ecb45340e0b3b2181b9da45b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#total_batches=200\n",
    "bar = tqdm_notebook(total=total_batches)\n",
    "for batch_number in range(total_batches):\n",
    "    \n",
    "    current_batch=data.next()\n",
    "    \n",
    "    first_image_number=current_batch[2][0]\n",
    "    \n",
    "    ids=np.array(current_batch[1].numpy())\n",
    "    if INCLUDE_FLIPPED_IMAGES:\n",
    "        ids=np.concatenate((ids,ids), axis=0)\n",
    "    \n",
    "    X_data=np.array(current_batch[0].numpy())\n",
    "    if INCLUDE_FLIPPED_IMAGES:\n",
    "        X_data = np.concatenate((X_data, X_data[:,:,::-1,:]), axis=0)\n",
    "    \n",
    "    ## Predictions of model\n",
    "    predictions  = model.predict(X_data, verbose=0)\n",
    "\n",
    "    # if it is a first batch, simply put predictions in buffer\n",
    "    if batch_number==0:\n",
    "        buffer_pred=predictions\n",
    "        buffer_id=ids\n",
    "\n",
    "    else:\n",
    "        #if first image number is zero\n",
    "        # 1. first process the items of buffer\n",
    "        if first_image_number == 0:\n",
    "            process_buffer(data=buffer_pred,  index=buffer_id)\n",
    "            #res_ids, res_classes=process_buffer(data=buffer_pred,  index=buffer_id)\n",
    "            #result_ids = np.concatenate((result_ids, res_ids), axis=0)\n",
    "            #result_classes = np.concatenate((result_classes, res_classes), axis=0)\n",
    "            \n",
    "            # 2. Then put only current predictions and ids in the buffers\n",
    "            buffer_pred=predictions\n",
    "            buffer_id=ids\n",
    "        else:\n",
    "            # if first image number is not zero simply append predictins and ids to buffer\n",
    "            buffer_pred = np.concatenate((buffer_pred, predictions), axis=0)\n",
    "            buffer_id = np.concatenate((buffer_id, ids), axis=0)\n",
    "    \n",
    "    if batch_number == total_batches-1:\n",
    "        process_buffer(data=buffer_pred,  index=buffer_id)\n",
    "        #res_ids, res_classes=process_buffer(data=buffer_pred,  index=buffer_id)\n",
    "        #result_ids = np.concatenate((result_ids, res_ids), axis=0)\n",
    "        #result_classes = np.concatenate((result_classes, res_classes), axis=0)\n",
    "    \n",
    "    del ids, predictions, X_data, current_batch\n",
    "    bar.update()\n",
    "    \n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
