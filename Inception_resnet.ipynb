{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import bson\n",
    "import struct\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvpr/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# First load the lookup tables from the CSV files\n",
    "\n",
    "categories_df = pd.read_csv(\"categories.csv\", index_col=0)\n",
    "\n",
    "## method borrord from part 01 of this notebook\n",
    "def make_category_tables():\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    for ir in categories_df.itertuples():\n",
    "        category_id = ir[0]\n",
    "        category_idx = ir[4]\n",
    "        cat2idx[category_id] = category_idx\n",
    "        idx2cat[category_idx] = category_id\n",
    "    return cat2idx, idx2cat\n",
    "##################################\n",
    "\n",
    "cat2idx, idx2cat = make_category_tables()\n",
    "\n",
    "train_offsets_df = pd.read_csv(\"train_offsets.csv\", index_col=0)\n",
    "train_images_df = pd.read_csv(\"train_images.csv\", index_col=0)\n",
    "val_images_df = pd.read_csv(\"val_images.csv\", index_col=0)\n",
    "\n",
    "test_offsets_df = pd.read_csv(\"test_offsets.csv\", index_col=0)\n",
    "test_images_df = pd.read_csv(\"test_images.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras generator is implemented by the BSONIterator class. It creates batches of images (and their one-hot encoded labels) directly from the BSON file. It can be used with multiple workers.\n",
    "\n",
    "Note: For fastest results, put the train.bson and test.bson files on a fast drive (SSD).\n",
    "\n",
    "See also the code in: https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "class BSONIterator(Iterator):\n",
    "    def __init__(self, bson_file, images_df, offsets_df, num_class,\n",
    "                 image_data_generator, lock, target_size=(180, 180), \n",
    "                 with_labels=True, batch_size=32, shuffle=False, seed=None):\n",
    "\n",
    "        self.file = bson_file\n",
    "        self.images_df = images_df\n",
    "        self.offsets_df = offsets_df\n",
    "        self.with_labels = with_labels\n",
    "        self.samples = len(images_df)\n",
    "        self.num_class = num_class\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "        self.image_shape = self.target_size + (3,)\n",
    "\n",
    "        print(\"Found %d images belonging to %d classes.\" % (self.samples, self.num_class))\n",
    "\n",
    "        super(BSONIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n",
    "        self.lock = lock\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        if self.with_labels:\n",
    "            batch_y = np.zeros((len(batch_x), self.num_class), dtype=K.floatx())\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            # Protect file and dataframe access with a lock.\n",
    "            with self.lock:\n",
    "                image_row = self.images_df.iloc[j]\n",
    "                product_id = image_row[\"product_id\"]\n",
    "                offset_row = self.offsets_df.loc[product_id]\n",
    "\n",
    "                # Read this product's data from the BSON file.\n",
    "                self.file.seek(offset_row[\"offset\"])\n",
    "                item_data = self.file.read(offset_row[\"length\"])\n",
    "\n",
    "            # Grab the image from the product.\n",
    "            item = bson.BSON.decode(item_data)\n",
    "            img_idx = image_row[\"img_idx\"]\n",
    "            bson_img = item[\"imgs\"][img_idx][\"picture\"]\n",
    "\n",
    "            # Load the image.\n",
    "            img = load_img(io.BytesIO(bson_img), target_size=self.target_size)\n",
    "\n",
    "            # Preprocess the image.\n",
    "            x = img_to_array(img)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            # Add the image and the label to the batch (one-hot encoded).\n",
    "            batch_x[i] = x\n",
    "            if self.with_labels:\n",
    "                batch_y[i, image_row[\"category_idx\"]] = 1\n",
    "\n",
    "        if self.with_labels:\n",
    "            return batch_x, batch_y\n",
    "        else:\n",
    "            return batch_x\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## borrowed from part 01\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "train_bson_path = os.path.join(data_dir, \"train.bson\")\n",
    "num_train_products = 7069896\n",
    "\n",
    "# train_bson_path = os.path.join(data_dir, \"train_example.bson\")\n",
    "# num_train_products = 82\n",
    "\n",
    "test_bson_path = os.path.join(data_dir, \"test.bson\")\n",
    "num_test_products = 1768182\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bson_file = open(train_bson_path, \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generator for training and a generator for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_img(x):\n",
    "    '''\n",
    "        converts RGB values from [0,255] to [-1, 1]\n",
    "    '''\n",
    "    return ((x/255.0)-0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11137481 images belonging to 5270 classes.\n",
      "Found 1233812 images belonging to 5270 classes.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5270\n",
    "num_train_images = len(train_images_df)\n",
    "num_val_images = len(val_images_df)\n",
    "batch_size = 256\n",
    "\n",
    "# Tip: use ImageDataGenerator for data augmentation and preprocessing.\n",
    "train_datagen = ImageDataGenerator( preprocessing_function=preprocess_img,\n",
    "                                    horizontal_flip=True)\n",
    "train_gen = BSONIterator(train_bson_file, train_images_df, train_offsets_df, \n",
    "                         num_classes, train_datagen, lock,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_img)\n",
    "val_gen = BSONIterator(train_bson_file, val_images_df, train_offsets_df,\n",
    "                       num_classes, val_datagen, lock,\n",
    "                       batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How fast is the generator? Create a single batch:\n",
    "next(train_gen)  # warm-up\n",
    "%time bx, by = next(train_gen) #bx=image_batch, by=label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it really output images and one-hot encoded class labels? \n",
    "\n",
    "Note that the images are pre-processed (and augmented) and therefore may look weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_preprocess_img(x):\n",
    "    return ((x / 2.0)+0.5)*255.0\n",
    "\n",
    "plt.imshow(bx[-1].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_idx = np.argmax(by[-1])\n",
    "cat_id = idx2cat[cat_idx]\n",
    "categories_df.loc[cat_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time bx, by = next(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(bx[-1].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_idx = np.argmax(by[-1])\n",
    "cat_id = idx2cat[cat_idx]\n",
    "categories_df.loc[cat_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = InceptionResNetV2(include_top=False, pooling='avg',input_shape=(180, 180, 3))\n",
    "outputs = Dense(num_classes, activation='softmax')(base_model.output)\n",
    "model = Model(base_model.inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training last 20 layers \n",
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To train the model:\n",
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 18)\n",
    "model.save('Test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Epoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To train the model:\n",
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 18)\n",
    "model.save('Inception_resnetv2_epoch2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 18)\n",
    "model.save('Inception_resnetv2_epoch3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training last 36 layers \n",
    "for layer in model.layers[-36:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 16)\n",
    "model.save('Inception_resnetv2_epoch4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "model = load_model('Inception_resnetv2_epoch4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training last 52 layers \n",
    "for layer in model.layers[-52:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 16)\n",
    "model.save('Inception_resnetv2_epoch5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history.history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate is changed to 0.001\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 16)\n",
    "model.save('Inception_resnetv2_epoch6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Epoch 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "model = load_model('Inception_resnetv2_epoch6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training last 68 layers \n",
    "#for layer in model.layers:\n",
    "#    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "adm = Adam(lr=1e-4)\n",
    "#sgd = SGD(lr=1e-4, decay=1e-4, momentum=0.99, nesterov=True)\n",
    "model.compile(optimizer=adm, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b718a7a3e2845b4a3e2d84d34687b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acabd5dc76eb484189bdf2c773ed19fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 8)\n",
    "model.save('Inception_resnetv2_epoch7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.65477956819753036],\n",
       " 'loss': [1.5922572999805251],\n",
       " 'val_acc': [0.62854713684751506],\n",
       " 'val_loss': [1.8964916627906778]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36f285066364cc68f70bb97d60ac7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adec2f7ba4a426ca5ee698e844cd499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 8)\n",
    "model.save('Inception_resnetv2_epoch8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.66236467653672026],\n",
       " 'loss': [1.5535655244428412],\n",
       " 'val_acc': [0.6304096572324619],\n",
       " 'val_loss': [1.89411363175001]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191972260d2a487d8fb6cca5c2ec27f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5049a643edb04120bb077aa067ae1906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 16)\n",
    "model.save('Inception_resnetv2_epoch9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.66602726415427138],\n",
       " 'loss': [1.5329860846091261],\n",
       " 'val_acc': [0.6309648471672451],\n",
       " 'val_loss': [1.8863668545084886]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "adm = Adam(lr=1e-5)\n",
    "#sgd = SGD(lr=1e-4, decay=1e-4, momentum=0.99, nesterov=True)\n",
    "model.compile(optimizer=adm, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c318f3246aea44499f60c90b673c8d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd51ae4a39b8484ebd170e0264cf75ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 16)\n",
    "model.save('Inception_resnetv2_epoch10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.67101142529447899],\n",
       " 'loss': [1.5052684286745723],\n",
       " 'val_acc': [0.63201606079428252],\n",
       " 'val_loss': [1.8849930250323099]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "model = load_model('Inception_resnetv2_epoch10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0, \n",
    "                    workers = 8)\n",
    "model.save('Inception_resnetv2_epoch11.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bson_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet 152\n",
    "source: https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from custom_layers.scale_layer import Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    '''The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=False)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter2, (kernel_size, kernel_size),\n",
    "                      name=conv_name_base + '2b', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    x = merge([x, input_tensor], mode='sum', name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    '''conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), subsample=strides,\n",
    "                      name=conv_name_base + '2a', use_bias=False)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter2, (kernel_size, kernel_size),\n",
    "                      name=conv_name_base + '2b', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(nb_filter3, (1, 1), subsample=strides,\n",
    "                             name=conv_name_base + '1', use_bias=False)(input_tensor)\n",
    "    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)\n",
    "\n",
    "    x = merge([x, shortcut], mode='sum', name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet152_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
    "    \"\"\"\n",
    "    Resnet 152 Model for Keras\n",
    "\n",
    "    Model Schema and layer naming follow that of the original Caffe implementation\n",
    "    https://github.com/KaimingHe/deep-residual-networks\n",
    "\n",
    "    ImageNet Pretrained Weights \n",
    "    Theano: https://drive.google.com/file/d/0Byy2AcGyEVxfZHhUT3lWVWxRN28/view?usp=sharing\n",
    "    TensorFlow: https://drive.google.com/file/d/0Byy2AcGyEVxfeXExMzNNOHpEODg/view?usp=sharing\n",
    "\n",
    "    Parameters:\n",
    "      img_rows, img_cols - resolution of inputs\n",
    "      channel - 1 for grayscale, 3 for color \n",
    "      num_classes - number of class labels for our classification task\n",
    "    \"\"\"\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        bn_axis = 3\n",
    "        img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        img_input = Input(shape=(color_type, img_rows, img_cols), name='data')\n",
    "\n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Conv2D(64, (7, 7), subsample=(2, 2), name='conv1', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Scale(axis=bn_axis, name='scale_conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    for i in range(1,8):\n",
    "        x = identity_block(x, 3, [128, 128, 512], stage=3, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    for i in range(1,36):\n",
    "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    x_fc = GlobalAveragePooling2D(data_format=\"channels_last\", name='global_avg_pool')(x)\n",
    "    #x_fc = Flatten()(x_fc)\n",
    "    #x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n",
    "\n",
    "    model = Model(img_input, x_fc)\n",
    "\n",
    "    #if K.image_dim_ordering() == 'th':\n",
    "    #  # Use pre-trained weights for Theano backend\n",
    "    #  weights_path = 'imagenet_models/resnet152_weights_th.h5'\n",
    "    #else:\n",
    "    #  # Use pre-trained weights for Tensorflow backend\n",
    "    #  weights_path = 'imagenet_models/resnet152_weights_tf.h5'\n",
    "    weights_path = 'imagenet_models/resnet152_weights_tf.h5'\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # Truncate and replace softmax layer for transfer learning\n",
    "    # Cannot use model.layers.pop() since model is not of Sequential() type\n",
    "    # The method below works since pre-trained weights are stored in layers but not in the model\n",
    "    x_newfc = GlobalAveragePooling2D(data_format=\"channels_last\", name='global_avg_pool')(x)\n",
    "    #x_newfc = Flatten()(x_newfc)\n",
    "    x_newfc = Dense(num_classes, activation='softmax', name='fc8')(x_newfc)\n",
    "\n",
    "    model = Model(img_input, x_newfc)\n",
    "    \n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows=180\n",
    "img_cols=180\n",
    "img_channels=3\n",
    "num_classes=5270\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load our model\n",
    "model = resnet152_model(img_rows, img_cols, img_channels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To train the model:\n",
    "train_history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch =math.ceil(train_gen.n/batch_size),\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = math.ceil(val_gen.n/batch_size),\n",
    "                    workers = 18)\n",
    "model.save('resnet152.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
